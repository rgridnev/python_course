{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from scipy.misc import derivative\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "iris = datasets.load_iris()\n",
    "# iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl</th>\n",
       "      <th>sw</th>\n",
       "      <th>pl</th>\n",
       "      <th>pw</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sl   sw   pl   pw  type\n",
       "0   5.1  3.5  1.4  0.2     0\n",
       "1   4.9  3.0  1.4  0.2     0\n",
       "2   4.7  3.2  1.3  0.2     0\n",
       "3   4.6  3.1  1.5  0.2     0\n",
       "4   5.0  3.6  1.4  0.2     0\n",
       "..  ...  ...  ...  ...   ...\n",
       "95  5.7  3.0  4.2  1.2     1\n",
       "96  5.7  2.9  4.2  1.3     1\n",
       "97  6.2  2.9  4.3  1.3     1\n",
       "98  5.1  2.5  3.0  1.1     1\n",
       "99  5.7  2.8  4.1  1.3     1\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# собираем из массива датасет, обозначаем столбцы для удобства работы, убираем лишний тип согласно условиям задачи\n",
    "data = pd.DataFrame(iris['data'])\n",
    "target = pd.DataFrame(iris['target'])\n",
    "dt = pd.concat([data, target], axis =1)\n",
    "dt.columns = ['sl', 'sw', 'pl', 'pw', 'type']\n",
    "dt = dt.loc[dt['type'] != 2]\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dt.drop('type', axis = 1)\n",
    "Y = dt['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class OptimizerBase(ABC):\n",
    "    def __init__(self, lr, scheduler=None):\n",
    "        \"\"\"\n",
    "        An abstract base class for all Optimizer objects.\n",
    "        This should never be used directly.\n",
    "        \"\"\"\n",
    "        from ..initializers import SchedulerInitializer\n",
    "\n",
    "        self.cache = {}\n",
    "        self.cur_step = 0\n",
    "        self.hyperparameters = {}\n",
    "        self.lr_scheduler = SchedulerInitializer(scheduler, lr=lr)()\n",
    "\n",
    "    def __call__(self, param, param_grad, param_name, cur_loss=None):\n",
    "        return self.update(param, param_grad, param_name, cur_loss)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Increment the optimizer step counter by 1\"\"\"\n",
    "        self.cur_step += 1\n",
    "\n",
    "    def reset_step(self):\n",
    "        \"\"\"Reset the step counter to zero\"\"\"\n",
    "        self.cur_step = 0\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Return a copy of the optimizer object\"\"\"\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def set_params(self, hparam_dict=None, cache_dict=None):\n",
    "        \"\"\"Set the parameters of the optimizer object from a dictionary\"\"\"\n",
    "        from ..initializers import SchedulerInitializer\n",
    "\n",
    "        if hparam_dict is not None:\n",
    "            for k, v in hparam_dict.items():\n",
    "                if k in self.hyperparameters:\n",
    "                    self.hyperparameters[k] = v\n",
    "                    if k == \"lr_scheduler\":\n",
    "                        self.lr_scheduler = SchedulerInitializer(v, lr=None)()\n",
    "\n",
    "        if cache_dict is not None:\n",
    "            for k, v in cache_dict.items():\n",
    "                if k in self.cache:\n",
    "                    self.cache[k] = v\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, param, param_grad, param_name, cur_loss=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SGD(OptimizerBase):\n",
    "    def __init__(\n",
    "        self, lr=0.01, momentum=0.0, clip_norm=None, lr_scheduler=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A stochastic gradient descent optimizer.\n",
    "        Notes\n",
    "        -----\n",
    "        For model parameters :math:`\\\\theta`, averaged parameter gradients\n",
    "        :math:`\\\\nabla_{\\\\theta} \\mathcal{L}`, and learning rate :math:`\\eta`,\n",
    "        the SGD update at timestep `t` is\n",
    "        .. math::\n",
    "            \\\\text{update}^{(t)}\n",
    "                &=  \\\\text{momentum} \\cdot \\\\text{update}^{(t-1)} + \\eta^{(t)} \\\\nabla_{\\\\theta} \\mathcal{L}\\\\\\\\\n",
    "            \\\\theta^{(t+1)}\n",
    "                &\\leftarrow  \\\\theta^{(t)} - \\\\text{update}^{(t)}\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            Learning rate for SGD. If scheduler is not None, this is used as\n",
    "            the starting learning rate. Default is 0.01.\n",
    "        momentum : float in range [0, 1]\n",
    "            The fraction of the previous update to add to the current update.\n",
    "            If 0, no momentum is applied. Default is 0.\n",
    "        clip_norm : float\n",
    "            If not None, all param gradients are scaled to have maximum l2 norm of\n",
    "            `clip_norm` before computing update. Default is None.\n",
    "        lr_scheduler : str, :doc:`Scheduler <numpy_ml.neural_nets.schedulers>` object, or None\n",
    "            The learning rate scheduler. If None, use a constant learning\n",
    "            rate equal to `lr`. Default is None.\n",
    "        \"\"\"\n",
    "        super().__init__(lr, lr_scheduler)\n",
    "\n",
    "        self.hyperparameters = {\n",
    "            \"id\": \"SGD\",\n",
    "            \"lr\": lr,\n",
    "            \"momentum\": momentum,\n",
    "            \"clip_norm\": clip_norm,\n",
    "            \"lr_scheduler\": str(self.lr_scheduler),\n",
    "        }\n",
    "\n",
    "    def __str__(self):\n",
    "        H = self.hyperparameters\n",
    "        lr, mm, cn, sc = H[\"lr\"], H[\"momentum\"], H[\"clip_norm\"], H[\"lr_scheduler\"]\n",
    "        return \"SGD(lr={}, momentum={}, clip_norm={}, lr_scheduler={})\".format(\n",
    "            lr, mm, cn, sc\n",
    "        )\n",
    "\n",
    "    def update(self, param, param_grad, param_name, cur_loss=None):\n",
    "        \"\"\"\n",
    "        Compute the SGD update for a given parameter\n",
    "        Parameters\n",
    "        ----------\n",
    "        param : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of the parameter to be updated.\n",
    "        param_grad : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The gradient of the loss function with respect to `param_name`.\n",
    "        param_name : str\n",
    "            The name of the parameter.\n",
    "        cur_loss : float\n",
    "            The training or validation loss for the current minibatch. Used for\n",
    "            learning rate scheduling e.g., by\n",
    "            :class:`~numpy_ml.neural_nets.schedulers.KingScheduler`.\n",
    "            Default is None.\n",
    "        Returns\n",
    "        -------\n",
    "        updated_params : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of `param` after applying the momentum update.\n",
    "        \"\"\"\n",
    "        C = self.cache\n",
    "        H = self.hyperparameters\n",
    "        momentum, clip_norm = H[\"momentum\"], H[\"clip_norm\"]\n",
    "        lr = self.lr_scheduler(self.cur_step, cur_loss)\n",
    "\n",
    "        if param_name not in C:\n",
    "            C[param_name] = np.zeros_like(param_grad)\n",
    "\n",
    "        # scale gradient to avoid explosion\n",
    "        t = np.inf if clip_norm is None else clip_norm\n",
    "        if norm(param_grad) > t:\n",
    "            param_grad = param_grad * t / norm(param_grad)\n",
    "\n",
    "        update = momentum * C[param_name] + lr * param_grad\n",
    "        self.cache[param_name] = update\n",
    "        return param - update\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#                      Adaptive Gradient Methods                      #\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RMSProp(OptimizerBase):\n",
    "    def __init__(\n",
    "        self, lr=0.001, decay=0.9, eps=1e-7, clip_norm=None, lr_scheduler=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        RMSProp optimizer.\n",
    "        Notes\n",
    "        -----\n",
    "        RMSProp was proposed as a refinement of :class:`AdaGrad` to reduce its\n",
    "        aggressive, monotonically decreasing learning rate.\n",
    "        RMSProp uses a *decaying average* of the previous squared gradients\n",
    "        (second moment) rather than just the immediately preceding squared\n",
    "        gradient for its `previous_update` value.\n",
    "        Equations::\n",
    "            cache[t] = decay * cache[t-1] + (1 - decay) * grad[t] ** 2\n",
    "            update[t] = lr * grad[t] / (np.sqrt(cache[t]) + eps)\n",
    "            param[t+1] = param[t] - update[t]\n",
    "        Note that the ``**`` and ``/`` operations are elementwise.\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            Learning rate for update. Default is 0.001.\n",
    "        decay : float in [0, 1]\n",
    "            Rate of decay for the moving average. Typical values are [0.9,\n",
    "            0.99, 0.999]. Default is 0.9.\n",
    "        eps : float\n",
    "            Constant term to avoid divide-by-zero errors during the update calc. Default is 1e-7.\n",
    "        clip_norm : float or None\n",
    "            If not None, all param gradients are scaled to have maximum l2 norm of\n",
    "            `clip_norm` before computing update. Default is None.\n",
    "        lr_scheduler : str or :doc:`Scheduler <numpy_ml.neural_nets.schedulers>` object or None\n",
    "            The learning rate scheduler. If None, use a constant learning\n",
    "            rate equal to `lr`. Default is None.\n",
    "        \"\"\"\n",
    "        super().__init__(lr, lr_scheduler)\n",
    "\n",
    "        self.cache = {}\n",
    "        self.hyperparameters = {\n",
    "            \"id\": \"RMSProp\",\n",
    "            \"lr\": lr,\n",
    "            \"eps\": eps,\n",
    "            \"decay\": decay,\n",
    "            \"clip_norm\": clip_norm,\n",
    "            \"lr_scheduler\": str(self.lr_scheduler),\n",
    "        }\n",
    "\n",
    "    def __str__(self):\n",
    "        H = self.hyperparameters\n",
    "        sc = H[\"lr_scheduler\"]\n",
    "        lr, eps, dc, cn = H[\"lr\"], H[\"eps\"], H[\"decay\"], H[\"clip_norm\"]\n",
    "        return \"RMSProp(lr={}, eps={}, decay={}, clip_norm={}, lr_scheduler={})\".format(\n",
    "            lr, eps, dc, cn, sc\n",
    "        )\n",
    "\n",
    "    def update(self, param, param_grad, param_name, cur_loss=None):\n",
    "        \"\"\"\n",
    "        Compute the RMSProp update for a given parameter.\n",
    "        Parameters\n",
    "        ----------\n",
    "        param : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of the parameter to be updated\n",
    "        param_grad : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The gradient of the loss function with respect to `param_name`\n",
    "        param_name : str\n",
    "            The name of the parameter\n",
    "        cur_loss : float or None\n",
    "            The training or validation loss for the current minibatch. Used for\n",
    "            learning rate scheduling e.g., by\n",
    "            :class:`~numpy_ml.neural_nets.schedulers.KingScheduler`.\n",
    "            Default is None.\n",
    "        Returns\n",
    "        -------\n",
    "        updated_params : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of `param` after applying the RMSProp update.\n",
    "        \"\"\"\n",
    "        C = self.cache\n",
    "        H = self.hyperparameters\n",
    "        eps, decay, clip_norm = H[\"eps\"], H[\"decay\"], H[\"clip_norm\"]\n",
    "        lr = self.lr_scheduler(self.cur_step, cur_loss)\n",
    "\n",
    "        if param_name not in C:\n",
    "            C[param_name] = np.zeros_like(param_grad)\n",
    "\n",
    "        # scale gradient to avoid explosion\n",
    "        t = np.inf if clip_norm is None else clip_norm\n",
    "        if norm(param_grad) > t:\n",
    "            param_grad = param_grad * t / norm(param_grad)\n",
    "\n",
    "        C[param_name] = decay * C[param_name] + (1 - decay) * param_grad ** 2\n",
    "        update = lr * param_grad / (np.sqrt(C[param_name]) + eps)\n",
    "        self.cache = C\n",
    "        return param - update\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, penalty=\"l2\", gamma=0, fit_intercept=True):\n",
    "        r\"\"\"\n",
    "        A simple logistic regression model fit via gradient descent on the\n",
    "        penalized negative log likelihood.\n",
    "        Notes\n",
    "        -----\n",
    "        For logistic regression, the penalized negative log likelihood of the\n",
    "        targets **y** under the current model is\n",
    "        .. math::\n",
    "            - \\log \\mathcal{L}(\\mathbf{b}, \\mathbf{y}) = -\\frac{1}{N} \\left[\n",
    "                \\left(\n",
    "                    \\sum_{i=0}^N y_i \\log(\\hat{y}_i) +\n",
    "                      (1-y_i) \\log(1-\\hat{y}_i)\n",
    "                \\right) - R(\\mathbf{b}, \\gamma) \n",
    "            \\right]\n",
    "        \n",
    "        where\n",
    "        \n",
    "        .. math::\n",
    "        \n",
    "            R(\\mathbf{b}, \\gamma) = \\left\\{\n",
    "                \\begin{array}{lr}\n",
    "                    \\frac{\\gamma}{2} ||\\mathbf{beta}||_2^2 & :\\texttt{ penalty = 'l2'}\\\\\n",
    "                    \\gamma ||\\beta||_1 & :\\texttt{ penalty = 'l1'}\n",
    "                \\end{array}\n",
    "                \\right.\n",
    "                \n",
    "        is a regularization penalty, :math:`\\gamma` is a regularization weight, \n",
    "        `N` is the number of examples in **y**, and **b** is the vector of model \n",
    "        coefficients.\n",
    "        Parameters\n",
    "        ----------\n",
    "        penalty : {'l1', 'l2'}\n",
    "            The type of regularization penalty to apply on the coefficients\n",
    "            `beta`. Default is 'l2'.\n",
    "        gamma : float\n",
    "            The regularization weight. Larger values correspond to larger\n",
    "            regularization penalties, and a value of 0 indicates no penalty.\n",
    "            Default is 0.\n",
    "        fit_intercept : bool\n",
    "            Whether to fit an intercept term in addition to the coefficients in\n",
    "            b. If True, the estimates for `beta` will have `M + 1` dimensions,\n",
    "            where the first dimension corresponds to the intercept. Default is\n",
    "            True.\n",
    "        \"\"\"\n",
    "        err_msg = \"penalty must be 'l1' or 'l2', but got: {}\".format(penalty)\n",
    "        assert penalty in [\"l2\", \"l1\"], err_msg\n",
    "        self.beta = None\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, tol=1e-7, max_iter=1e7):\n",
    "        \"\"\"\n",
    "        Fit the regression coefficients via gradient descent on the negative\n",
    "        log likelihood.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            A dataset consisting of `N` examples, each of dimension `M`.\n",
    "        y : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            The binary targets for each of the `N` examples in `X`.\n",
    "        lr : float\n",
    "            The gradient descent learning rate. Default is 1e-7.\n",
    "        max_iter : float\n",
    "            The maximum number of iterations to run the gradient descent\n",
    "            solver. Default is 1e7.\n",
    "        \"\"\"\n",
    "        # convert X to a design matrix if we're fitting an intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        l_prev = np.inf\n",
    "        self.beta = np.random.rand(X.shape[1])\n",
    "        for _ in range(int(max_iter)):\n",
    "            y_pred = sigmoid(np.dot(X, self.beta))\n",
    "            loss = self._NLL(X, y, y_pred)\n",
    "            if l_prev - loss < tol:\n",
    "                return\n",
    "            l_prev = loss\n",
    "            self.beta -= lr * self._NLL_grad(X, y, y_pred)\n",
    "\n",
    "    def _NLL(self, X, y, y_pred):\n",
    "        r\"\"\"\n",
    "        Penalized negative log likelihood of the targets under the current\n",
    "        model.\n",
    "        .. math::\n",
    "            \\text{NLL} = -\\frac{1}{N} \\left[\n",
    "                \\left(\n",
    "                    \\sum_{i=0}^N y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)\n",
    "                \\right) - R(\\mathbf{b}, \\gamma)\n",
    "            \\right]\n",
    "        \"\"\"\n",
    "        N, M = X.shape\n",
    "        beta, gamma = self.beta, self.gamma \n",
    "        order = 2 if self.penalty == \"l2\" else 1\n",
    "        norm_beta = np.linalg.norm(beta, ord=order)\n",
    "        \n",
    "        nll = -np.log(y_pred[y == 1]).sum() - np.log(1 - y_pred[y == 0]).sum()\n",
    "        penalty = (gamma / 2) * norm_beta ** 2 if order == 2 else gamma * norm_beta\n",
    "        return (penalty + nll) / N\n",
    "\n",
    "    def _NLL_grad(self, X, y, y_pred):\n",
    "        \"\"\"Gradient of the penalized negative log likelihood wrt beta\"\"\"\n",
    "        N, M = X.shape\n",
    "        l1norm = lambda x: np.linalg.norm(x, 1)  # noqa: E731\n",
    "        p, beta, gamma = self.penalty, self.beta, self.gamma\n",
    "        d_penalty = gamma * beta if p == \"l2\" else gamma * np.sign(beta)\n",
    "        return -(np.dot(y - y_pred, X) + d_penalty) / N\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to generate prediction probabilities on a new\n",
    "        collection of data points.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(Z, M)`\n",
    "            A dataset consisting of `Z` new examples, each of dimension `M`.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(Z,)`\n",
    "            The model prediction probabilities for the items in `X`.\n",
    "        \"\"\"\n",
    "        # convert X to a design matrix if we're fitting an intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return sigmoid(np.dot(X, self.beta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
